{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Company","text":"<p>We are an Estonia-based IT consulting and administration team specializing in OpenStack cloud, Ceph storage and VyOS firewall. We have long-term competence in managing on-prem cloud, shared storage and firewall clusters. We will back you up when the rubber hits the road, so you don't have to!  </p> Generic information <p>Business name:  Wirt O\u00dc e-mail:     info@wirt.ee Register code:  16986693 IBAN (LHV): EE867700771010370923  SSH public key</p>"},{"location":"notes/","title":"Overview","text":""},{"location":"notes/#four-pillars-for-it-sadness","title":"Four pillars for IT sadness","text":"<p>Warning - nerd content! This section contains opinionated technical notes from real life. It's okay to agree, not agree, or philosophically acknowledge writers' viewpoints. Despite that, some commented code snippets may be helpful. </p>"},{"location":"notes/#compute-nodes","title":"Compute nodes","text":"<p>If it breaks or gets its upgrade, it is used and abused over and over to get out of as many ticks per cycle as possible. It is not a pet, even if it paid your kid's kindergarten and schooling and your own leisure time. And if it is worked many times over its expected lifetime, you try to sell its parts to recover some utilization costs. Totally underrated piece of everything that ships with cardboard coffins.</p>"},{"location":"notes/#networks-and-overlays","title":"Networks and overlays","text":"<p>In the good old days, there was a VLAN. And that's it.  Now, everything is on top of every other thing. Everyone expects excellent dual-stack performance, and MTU keeps shrinking.  Networking is so SDN that we tend to separate networks into \"hard\" and \"soft\". Soft networking is generated, and a hard network is something you type in with your fingertips to the switch console. Luckily, the physical network is still what it was. Only high-speed optic power consumption has blown through the roof. </p>"},{"location":"notes/#storage-tiers","title":"Storage tiers","text":"<p>A not long time ago, there were local storage, RAID and NAS/SAN boxes for LUNs you hacked together with LVM to get perf numbers for your DBA, who hates you. Now, it's shared erasure encoded multiprotocol mesh with online upgrades and failure domains. And if it explodes, you can keep all the pieces. And DBA still complains about service time. Regarding storage tearing and archival: \"King Tape is dead. Long live the King Tape.\" Sometimes, if you need to Petabyte your backyard, vendor lock is not a bad thing. Speaking about open source, it's worth mentioning that things are complicated, and sometimes, support contracts or wise men from the mountaintop are needed.  On a bad day, You take a half-block size 4M object and go on a pilgrimage. If it's a bad day on steroids, you can consult with your system administrator, e.g. with yourself.</p>"},{"location":"notes/#chaos-monkeys-eg-system-administratorsdevops","title":"Chaos monkeys (e.g. system administrators/DevOps)","text":"<p>We, I, are still the same. Old busted hardware, biased problem assessments, gut-based technology selections behind L1 support and paywall, and no repo-based CI. But this gut-based decision process still beats state-of-the-art language models. Of course, this \"Thing\" is a good company for a man who was brick-walled in with IBM's mainframe.</p>"},{"location":"notes/compute/","title":"Compute","text":""},{"location":"notes/compute/#hyperconverged-infrastructure","title":"Hyperconverged infrastructure","text":"<p>Everyone wants their cake and wants to eat it too. Expecialy when you have around 100 cores per multisocet socet machine with few TB of RAM loaded with tens of TB NVMEs. Request to get a shared cluster of everything is not so uncommon.  </p>"},{"location":"notes/compute/#numa","title":"NUMA","text":"<p>To avoid interfering with different services, it seems plausible to sort the running process. It also disciplines rouge clients and helps service providers with QOS management. Also, if it's overcommitted, it's much easier to communicate why things are slow.  </p> <pre><code>ps aux  | grep libvirt  | grep -v grep | awk '{print$2}' | while read line; do \n    echo taskset -cp \"${vm0}-${numa0_end},${vm1}-${numa1_end},${vm2}-${numa2_end},${vm3}-${numa3_end}\" \"${line}\"; done | sh 1&gt;/dev/null\n\nvirsh list  | awk '/inst/ {print $2}' | while read l;do \n    for i in $(seq 0 $(virsh dominfo $l |awk '/CPU\\(/ {print$2-1}')); do \n        virsh vcpupin $l $i \"${vm0}-${numa0_end},${vm1}-${numa1_end},${vm2}-${numa2_end},${vm3}-${numa3_end}\" 1&gt;/dev/null\n        #virsh vcpupin $l $i \"1-255\" 1&gt;/dev/null\n    done \ndone\n</code></pre>"},{"location":"notes/compute/#be-not-so-nice","title":"Be not so nice","text":"<p>If it waits behind storage, then the system slows down. Prioritize shared storage proceses.</p> <pre><code>ps -e -o pid,uid,ppid,pri,ni,cmd | awk '/ceph-osd/  &amp;&amp;  !/-20/ &amp;&amp; !/awk/ {print \"renice -n -20 -g\",$1,\";ionice -c 1 -n 1 -p\",$1}'  | sh\n</code></pre>"},{"location":"notes/network/","title":"Network","text":""},{"location":"notes/network/#ip-commands","title":"IP commands","text":"<p>It tends to be universal across Linux systems. If nothing else works, it is a viable option to shut down the local network manager daemon and do the configuration manually.  </p>"},{"location":"notes/network/#policy-based-routing","title":"Policy based routing","text":"<p>Sometimes machine has more than one interface. Then, it's time to decide what goes where:</p> <pre><code>ip addr add ${ip}/${mask} dev ${dev}\nip link set dev ${dev} up\ntest -z \"$(grep ut_pub /etc/iproute2/rt_tables)\" &amp;&amp; echo \"666 pub\" &gt;&gt; /etc/iproute2/rt_tables\nip rule add from ${ip} table pub\nip rule add to ${ip} table pub\nip route add default via ${gw} dev ${dev} table pub\n</code></pre>"},{"location":"notes/network/#martians","title":"Martians","text":"<p>If traffic to the interface is unexpected, you may get a silent drop. To avoid this drop, some relaxation and breathwork are needed:  </p> <pre><code>echo 1 &gt; /proc/sys/net/ipv4/conf/bond0/log_martians\nsysctl -w net.ipv4.conf.bond0.rp_filter=2\n</code></pre>"},{"location":"notes/network/#systemd-networkd","title":"Systemd networkd","text":"<p>Over the years, every new distro upgrade has introduced some new ways to configure your network.  The situation was so bad that adding ip commands directly to the crontab @reboot line was quite common.  It was a pleasant surprise that the networkd actually worked and did not cause brain damage when configured. </p> <pre><code>cat  /etc/systemd/network/bond.network\n[Match]\nName=eno3*\n\n[Network]\nBond=bond0\n\ncat /etc/systemd/network/bond0.netdev\n[NetDev]\nName=bond0\nDescription=LAG/Bond to a switch\nKind=bond\nMACAddress=aa:aa:aa:aa:aa:aa\n\n[Bond]\nMode=802.3ad\nMIIMonitorSec=1\nLACPTransmitRate=slow\n\ncat /etc/systemd/network/bond0.network \n[Match]\nName=bond0\n\n[Network]\nVLAN=bond0.&lt;vlan tag&gt;\n\ncat /etc/systemd/network/bond0.&lt;vlan tag&gt;.netdev \n[NetDev]\nName=bond0.&lt;vlan tag&gt;\nKind=vlan\n\n[VLAN]\nId=&lt;vlan tag&gt;\n\ncat /etc/systemd/network/bond0.&lt;vlan tag&gt;.network\n[Match]\nName=bond0.&lt;vlan tag&gt;\n\n[Network]\nDHCP=yes\n</code></pre>"},{"location":"notes/network/#systemd-hooks","title":"Systemd hooks","text":"<p>Sometimes, you need random scripts to run when the network connection status changes. The tool for the job in systemd world is networkd-dispatcher.</p> <pre><code>#!/bin/bash\n#workaroud for issue networkd backed netpaln that drops policy based routing and vlanX carring interface from br-vlan\n\nexport PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\n\nbrctl addif br-vlan bond0\nif [[ -z \"$(ip rule list | grep br-public1)\" ]]; then\n    ip rule add from xxx.xx.xx.0/24 table br-public1\n    ip route add default via xxx.xx.xx.1 dev br-public table br-public1\n    ip rule add from yyy.yy.yy.0/24 table br-public2\n        ip route add default via yyy.yy.yy.1 dev br-public table br-public2\n    ip route flush cache\nfi\n</code></pre>"},{"location":"notes/storage/","title":"Storage","text":""},{"location":"notes/storage/#disk-destroyer","title":"Disk Destroyer","text":"<p>There is no better tool to arrange your bits to oblivian. But if a precise cut is needed, some commands are more valuable than others. If your specific block size calculation is incorrect, you will lose your data!</p>"},{"location":"notes/storage/#raid","title":"RAID","text":"<p>Metadata information is usually at the end of the device. If you want to re-use the device, you need to wipe it out. You have the option to wait and overwrite the entire disk:</p> <pre><code>root@demo:~# dd if=/dev/zero of=/dev/sdX bs=1M status=progress\n</code></pre> <p>Or zero out only parts Raid controller bothers to check:</p> <pre><code>root@demo:~# DEV='/dev/sdX'; dd if=/dev/zero of=$DEV bs=512 seek=$(( $(blockdev --getsz $DEV) - 1024 )) count=1024\n</code></pre>"},{"location":"notes/storage/#software-raid-and-uefi","title":"Software RAID and UEFI","text":"<p>If RAID card is not present but you need FAT for EFI boot and some redundancy:  </p> <pre><code>/dev/sda1 is efi and /dev/sdb1 is empty\nroot@demo:~# dd if=/dev/sda1 of=/dev/sdb1\nroot@demo:~# mkdir /boot/efi2\nroot@demo:~# blkid /dev/sda1\n/dev/sda1: UUID=\"xxxx-xxxx\" TYPE=\"vfat\" PARTUUID=\"xxxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxx\"\nroot@demo:~# blkid /dev/sdb1\n/dev/sdb1: UUID=\"xxxx-xxxx\" TYPE=\"vfat\" PARTUUID=\"yyyyyyyyy-yyyy-yyyy-yyyy-yyyyyyyy\"\nroot@demo:~# cat /etc/fstab  | grep efi\nPARTUUID=xxxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxx  /boot/efi       vfat    umask=0077      0       1\nPARTUUID=xxxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxx  /boot/efi2      vfat    umask=0077      0       1\nroot@demo:~# efibootmgr -c -d /dev/sdb -p 1 -L \"Ubuntu-AltDrv\" -l '\\EFI\\ubuntu\\shimx64.efi'\n</code></pre>"},{"location":"notes/storage/#copy-block-device-over-network","title":"Copy block device over network","text":"<p>Let's assume you already have a healthy dose of SSH in your bloodstream. Copy block device /dev/sdX content from one machine to another machine /dev/sdY with the command:</p> <pre><code>root@demo:~# ssh root@source.example \"dd if=/dev/sdX\" bs=1M | dd of=/dev/sdY bs=1M\n</code></pre>"},{"location":"notes/storage/#swap-to-the-resque","title":"Swap to the resque","text":"<p>If you constantly get OOM killed and are happy to run your system randomly on glacier speed, then you can virtually add some RAM on the fly. 'pv' may be needed to avoid killing already slow storage. Example how to add 32G to swap:   </p> <pre><code>root@demo:~# dd if=/dev/zero | pv --rate-limit 60m | dd iflag=fullblock of=/swapfile bs=1G count=32 #32G swap\nroot@demo:~# dd if=/dev/zero of=/swapfile bs=4k count=8388608\nroot@demo:~# mkswap /swapfile\nroot@demo:~# chmod 0600 /swapfile\nroot@demo:~# /sbin/swapon /var/swapfile; #'swapoff' if needed\n</code></pre>"},{"location":"notes/storage/#get-ceph-backup","title":"Get CEPH backup","text":"<p>It is great when you have a backup or you don't destroy your network under an erasure-coded pool.</p> <pre><code>rbd export -p  pool volume-ad0ceef0-64ef-4050-afd6-3a12c13dd6be ./volume-ad0ceef0-64ef-4050-afd6-3a12c13dd6be.bac\n</code></pre>"},{"location":"notes/storage/#restore-ceph-from-backup","title":"Restore CEPH from backup","text":"<pre><code>rbd rm pool volume-ad0ceef0-64ef-4050-afd6-3a12c13dd6be\nrbd import ./volume-ad0ceef0-64ef-4050-afd6-3a12c13dd6be.bac  pool/volume-ad0ceef0-64ef-4050-afd6-3a12c13dd6be\n</code></pre>"},{"location":"notes/varia/","title":"Varia","text":""},{"location":"notes/varia/#verify-sha-256-checksum","title":"Verify SHA-256 checksum","text":"<p>To put an almost successful scam story short. Do not trust what you see! It may be a \"legitimate\" document from known sources. For the bare minimum, ask for data integrity verifications using the SHA-256 (SHA-2 family with a digest length of 256 bits). Data providers can so easily generate and publish hashes without even thinking about GDPR. Let's step through a simple example.</p> <p>First, generate some files with different content:</p> <pre><code>$ for i in {2345..2348};do date +\"%T.%6N\" &gt;  bill_${i}.pdf;done \n</code></pre> <p>Then shasum everithing:</p> <pre><code>$ sha256sum *\nc46dd8a87ecabd1e2003d08bb7e0e8702e18767d6b126f4f00ff79b95cc73276  bill_2345.pdf\na1f0219644c86e4490a0a87b86a1717322dfb67c8148cc5205ca4ce8ac64b54e  bill_2346.pdf\n3e8d7257bfa1ed995e2ceaf61404b7ab83ac978f62d0a8f09cb2e1b8ed35c181  bill_2347.pdf\n6e1f15409e50c5c1253197971a1f04c01fe456a284f6d6c9e4f4a98e5044e2d7  bill_2348.pdf\n</code></pre> <p>And then let's generate the publically shareable file ( via the company webpage):</p> <pre><code>sha256sum * &gt; january_bills.txt\n</code></pre> <p>Now, we can distribute those files via a not-so-secure channel. And if the user wants to check file authenticity, it's easily doable:</p> <pre><code>$ sha256sum bill_2345.pdf\nc46dd8a87ecabd1e2003d08bb7e0e8702e18767d6b126f4f00ff79b95cc73276  bill_2345.pdf\n</code></pre> <p>If the file is tampered, then result is NOT what you published on your company webpage:</p> <pre><code>$ echo \"tampering\" &gt;&gt;  bill_2345.pdf\n$ sha256sum bill_2345.pdf\n867b5be7d12023f2268f5c9124eb5a518852195019fd3f067322963724b1d5be  bill_2345.pdf\n</code></pre>"},{"location":"offers/","title":"Overview","text":""},{"location":"offers/#our-speciality","title":"Our speciality","text":"<ul> <li>OpenStack based cloud infrastructure deployment and administration at scale</li> <li>Ceph storage setup, maintenance and monitoring best practices</li> <li>VyOS firewall ruleset design and upgrade procedures</li> </ul>"},{"location":"offers/ceph/","title":"Ceph","text":""},{"location":"offers/ceph/#shared-vs-local-storage","title":"Shared vs. local storage","text":"<p>Local NVME storage is fast and finite in size. Shared storage allows moderate speedy space aggregation from all storage nodes, with the cost of events in one shared storage node, affecting the entire cluster. However, with shared storage, you can migrate VM online from one hypervisor to another.  </p>"},{"location":"offers/ceph/#placement","title":"Placement","text":"<p>Ceph fault tolerance depends on its disk's physical location. It is essential to figure out what fault tolerance assumptions are. Then, you can decide what you can lose (datacenter, rack, host, disk). It has to be a conscious decision.  </p>"},{"location":"offers/ceph/#replicated-vs-erasure-encoded","title":"Replicated vs. erasure encoded","text":"<p>Replicated is what the name says: \"Replicated X times.\" Generally, 3x is fine. Erasure encoding is an entirely different setup. On its basic level, a decision is needed on how many coding junks and data junks are required. Let's use K data junks and M coding junks. You need to use one coding junk for two data junks to set up a minimal supported EC pool. In literature, it is called the \"k=2 m=1\" schema.  </p>"},{"location":"offers/ceph/#fault-tolerance","title":"Fault tolerance","text":"<p>Let's take a trivial example. Cloud storage is 3x replicated. Losing one disk is not a significant event. Losing two disks prioritizes maximum recovery IOPS. Losing three disks means downtime and restoring from backup.</p>"},{"location":"offers/ceph/#block-vs-file-vs-object","title":"Block vs. file vs. object","text":"<p>Under the hood, it's an object storage. However, the translation layer allows it to present as a block, file or object. For OpenStack VM, it's a block device. For the journal log collector, it is a file. For S3 and Swift, it is an object.</p>"},{"location":"offers/openstack/","title":"OpenStack","text":""},{"location":"offers/openstack/#benefits-of-owning-an-on-prem-cloud-infrastructure","title":"Benefits of owning an on-prem cloud infrastructure","text":"<p>It's entirely under your control. The entire hardware and software lifecycle depends on your decisions. OpenStack itself, is set of services: compute, network, storage and controller plane (databases, proxies, identity, caching, UI, MQ).</p>"},{"location":"offers/openstack/#basic-on-prem-openstack-cloud-bundle","title":"Basic on-prem OpenStack cloud bundle","text":"<ul> <li>Server room with adequate power and cooling  </li> <li>Network stack  </li> <li>Compute, storage and controller nodes  </li> <li>OpenStack software installation  </li> <li>Plans for upgrade and disaster recovery  </li> </ul>"},{"location":"offers/openstack/#fault-tolerance","title":"Fault tolerance","text":"<p>From our point of view, a single piece of hardware is more resilient than a stack of rust. The same applies to software instances. But if it fails, then it's difficult or impossible to recover. On reasonable-sized OpenStack installation, something is always in a failed state or under maintenance. However, you need to know how fault-tolerant your installation is.  </p> <p>Let's look at two trivial examples. In the first example, the controller plane database is a 3x replica. Losing one replica is not a significant event. However, the split-brain condition can occur like any other quorum-based system. In the second example, the networking interface or switch malfunctions. Losing one network path won't affect installation. However, things would be sad if the design decision was to run a hyper-converged cloud infrastructure on a single interface.</p>"},{"location":"offers/openstack/#stack-of-all-things","title":"Stack of all things","text":"<p>As the name OpenStack suggests, it's a stack of things. We help you to make the correct choices in that stack.  </p>"},{"location":"offers/vyos/","title":"VyOS","text":""},{"location":"offers/vyos/#what-can-a-firewall-do-for-me","title":"What can a firewall do for me?","text":"<p>In its simplest form, it is a discriminator. If knocking comes from a bad neighbourhood or to the wrong door, you can ignore it - no need to be polite.  </p> <p>For illustration purposes, let's take a typical request from the person who wants to publish its service. \"Please open 172.16.42.42 TCP 9090 from 172.16.1.0/24.\"  </p> <p>or  </p> <p>\"Please open 172.16.1.0/24 TCP 9090 from 172.16.42.42.\"  </p> <p>From a firewall perspective, it is important to distinguish who initializes the connection. Requests from above mean very different things. The first case represents one specific hole for a single IP from a private range. However, in the second case, the entire /24 is open only from a single initiator. In conclusion, thinking through your source and destination is essential since it defines holes in your wall.</p>"},{"location":"offers/vyos/#routing","title":"Routing","text":"<p>Every host that needs to go somewhere needs to go its next hop. For that purpose, in 99% of cases, one IP from the network is reserved as the default gateway. It's a golden door to wild-wild-west, usually IP of the firewall itself. If a network packet from the host enters the firewall, it will be lifted from one interface to another, masked, filtered, or even dropped silently. Actions taken are entirely under the firewall administrator's will. Another essential thing may be the outgoing IP or interface from the firewall (SNAT) or DNAT if the public-facing IP serves some private IP.  </p>"},{"location":"offers/vyos/#redundancy","title":"Redundancy","text":"<p>From the previous chapter, readers can recall that the gateway is one single IP. It's obvious if it is single, then it is not replicated. For highly available services, avoiding a single point of failure is critical. Virtual Router Redundancy Protocol (VRRP) allows IP to move from a failed router to a live router. Usually, the hardest part is figuring out what alive or dead means.</p>"},{"location":"offers/vyos/#configuration","title":"Configuration","text":"<p>All installations are different. However, since the VyOS configuration is text-based, it seems reasonable to keep it in Git or some other versioning system. Deploying code via Ansible makes it less error-prone. Here is a trivial example snippet:   </p> <pre><code>- name: Global options\n  vyos_config:\n    lines:\n      - set firewall global-options all-ping enable\n</code></pre>"},{"location":"profile/","title":"Profile","text":""},{"location":"profile/#when-to-consider-us","title":"When to consider us","text":"<p>We are right pick in those situations where technical expertise is needed, but the requestor doesn't want to know anything about academia. Then we have a few options:  </p> <ul> <li>Convince business owners to work with academia</li> <li>Offer services for free (not likely, but one can hope) or denay request</li> <li>Use this compays governing body to provide the requested service   </li> </ul>"},{"location":"profile/#experience","title":"Experience","text":"<p>Our OpenStack cloud journey started in 2016 with the Mitaka release. Ceph storage utilisation started around 2017 with Jewel release. Mainly because commercial, supported solution IOPS was lower than expected.  VyOS HA firewall started in 2022 out of necessity from Equuleus. The previous corporate firewall support price was way beyond imagination. Since everything above is always live upgraded, we have solid recipes that work in real life.  </p>"},{"location":"profile/#team","title":"Team","text":"<p>Academia PhD graded, well paid technical support personnel. We politely refuse when a conflict of interest or massive academic infrastructure suits you better.  </p>"},{"location":"profile/#requests","title":"Requests","text":"<p>info@wirt.ee in English or Estonian.  </p>"},{"location":"profile/#pricing","title":"Pricing","text":"<p>Baseline is same as academia officials 60 \u20ac/h.</p>"}]}